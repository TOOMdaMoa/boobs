{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "import pprint\n",
    "\n",
    "## My libraries \n",
    "\n",
    "import comms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Algorithm settings **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This has to be set before generating dictionary ---------------------------------------------------------\n",
    "## How to count occurence of words -- method comms.create_dictionary argument {\"A\", \"B\"}\n",
    "counts_alg = \"A\"\n",
    "\n",
    "## How to generate batch -- method comms.generate_batch argument {\"A\"}\n",
    "genbatch_alg = \"A\"\n",
    "\n",
    "### This has to be set before running model and  ------------------------------------------------------------\n",
    "## Size of training batch\n",
    "batch_size = 32\n",
    "\n",
    "## Length of embedding vectors \n",
    "embedding_size = 32\n",
    "\n",
    "## Number of negative examples, see nonnegative sampling \n",
    "num_sampled = 16 \n",
    "\n",
    "## Number of iterations of optimization algorithm\n",
    "num_iter = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load data, create word_bags, split to train and test **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = comms.load_jsons() ## list of jsons, TESTING:    pprint.pprint(data[0], depth=1)\n",
    "user_item_df = comms.user_item_dataframe(data) ## dataframe [\"context\", \"word\"] = [\"user\", \"product\"]\n",
    "word_bags, context_ids = comms.create_contexts(user_item_df) ## list of lists of product_ids, list of user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = min(len(word_bags)//5, 100) ## number of users to make testing set of \n",
    "test_indices = np.random.choice(len(word_bags), test_size)\n",
    "train_indices = [i for i in range(len(word_bags)) if i not in test_indices]\n",
    "\n",
    "word_bags_test = [word_bags[i] for i in test_indices]\n",
    "word_bags_train = [word_bags[i] for i in train_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dictionary and batch generating **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, reversed_dictionary = comms.create_dictionary(word_bags_train, counts_alg)\n",
    "\n",
    "## Number of distinct words in vocabulary\n",
    "vocabulary_size = len(dictionary)\n",
    "\n",
    "genbatch_prereq = comms.create_genbatch_prerequisities(word_bags_train, dictionary)\n",
    "\n",
    "def generate_batch(batch_size): \n",
    "    return comms.generate_batch(batch_size, genbatch_prereq, genbatch_alg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build model and train it **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = comms.create_w2v_tf_model(batch_size, embedding_size, vocabulary_size, num_sampled)\n",
    "train_inputs, train_labels, embeddings, nce_weights, nce_biases, loss, optimizer, init = w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0  :  1.45273\n",
      "Loss at iteration 1000  :  1.59382\n",
      "Loss at iteration 2000  :  1.73079\n",
      "Loss at iteration 3000  :  1.47814\n",
      "Loss at iteration 4000  :  1.80039\n",
      "Loss at iteration 5000  :  1.57665\n",
      "Loss at iteration 6000  :  1.48287\n",
      "Loss at iteration 7000  :  1.6787\n",
      "Loss at iteration 8000  :  1.9216\n",
      "Loss at iteration 9000  :  1.45983\n"
     ]
    }
   ],
   "source": [
    "## Learn the embeddings\n",
    "for i in range(num_iter): \n",
    "    batch, labels = generate_batch(batch_size)\n",
    "    _, loss_eval = sess.run([optimizer, loss], {train_inputs : batch, train_labels : labels})\n",
    "    if i % 1000 == 0: \n",
    "        print(\"Loss at iteration\",i, \" : \",loss_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_eval, nce_weights_eval, nce_biases_eval = sess.run([embeddings, nce_weights, nce_biases]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Test the model on data ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take last word from each user \n",
    "## Try to predict it using the rest \n",
    "test_labels = [dictionary.get(wb[-1]) for wb in word_bags_test]\n",
    "test_batches = [[dictionary.get(w) for w in wb[:-1]] for wb in word_bags_test]\n",
    "\n",
    "## Remove Nones\n",
    "remove_indicators = [tl == None or all(x is None for x in tb) for tl,tb in zip(test_labels, test_batches)]\n",
    "\n",
    "test_labels_filter = [tl for tl, ri in zip(test_labels, remove_indicators) if not ri]\n",
    "test_batches_filter = [tb for tb, ri in zip(test_batches, remove_indicators) if not ri]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeds = [np.mean(embeddings_eval[tb, :], axis = 0) for tb in test_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_preferences = [comms.softmax(np.sum(nce_weights_eval * user_embed, axis = 1) + nce_biases_eval) for user_embed in user_embeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_of_selected_products = [(len(up) - np.argsort(np.argsort(up)))[ul] for up,ul in zip(user_preferences, test_labels_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 18]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We would present the items he actually selected in the following orders \n",
    "orders_of_selected_products "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
