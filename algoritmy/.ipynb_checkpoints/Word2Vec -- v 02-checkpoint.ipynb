{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Specify dimensions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_users = 128\n",
    "num_items = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Generate data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_items_per_user = 5\n",
    "data_len = avg_items_per_user * num_users\n",
    "\n",
    "user_indices = np.random.randint(0, num_users, size = data_len)\n",
    "item_indices = np.random.randint(0, num_items, size = data_len)\n",
    "\n",
    "user_item_df = pd.DataFrame({\"user\" : user_indices, \"item\" : item_indices})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Transform the data to list of item - item pairs **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouping = user_item_df.groupby(\"user\")\n",
    "contexts = grouping.item.apply(list)\n",
    "## Create list of [list of id items for one user]\n",
    "contexts = [c for c in contexts if len(c) > 1] \n",
    "## Create all i,j item pairs i!=j that occur together in some context\n",
    "item_context_pairs = [(i, j) for c in contexts for i in c for j in c if i != j] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create item dictionary **\n",
    "Question is how to count occurence of each item, there are two options \n",
    "- a) by number of users that viewed it  \n",
    "- b) by frequency in word_context_pairs -- this will higly push upward items that viewed someone who viewed a lot of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## option a) \n",
    "items_by_count = list(user_item_df[\"item\"].value_counts().keys()) ## items with highest counts come first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## option b) \n",
    "item_context_pairs_flat = [item for pair in item_context_pairs for item in pair]\n",
    "items_by_count = [pair[0] for pair in collections.Counter(item_context_pairs_flat).most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = dict(zip(items_by_count, range(len(items_by_count))))\n",
    "reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Warning **\n",
    "Till now we worked with items as strings/ints - from now on items will be marked only by index in dictionary (which has format \"item_id\" : item index -- here the item index corresponds to one of the orderings of items defined above and IS important for the algorithm to work properly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lets write functions to generate our batches for training **\n",
    "\n",
    "Again, we can use more refined sampling (but slower and maybe not neccessarily better) or simpler and faster one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ic_pairs_len = len(item_context_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[285, 868, 261, 277, 11, 829, 725, 463, 493, 97]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.random.choice(a = ic_pairs_len, size = 10, replace = False)\n",
    "[item_context_pairs[i][0] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size): \n",
    "    \"\"\"\n",
    "    Generates batch, labels (as in tf function)\n",
    "    Primitive version -- not taking into account different context sizes \n",
    "\n",
    "    Arguments: \n",
    "    batch_size -- size of the batch\n",
    "    \n",
    "    Return: \n",
    "    batch -- words from which we try to predict labels\n",
    "    labels -- words that we are trying to predict from batch\n",
    "    \n",
    "    \"\"\"\n",
    "    indices = np.random.choice(a = ic_pairs_len, size = batch_size, replace = False)\n",
    "    batch = np.array([dictionary[item_context_pairs[i][0]] for i in indices])\n",
    "    labels = np.array([dictionary[item_context_pairs[i][1]] for i in indices])\n",
    "    return batch, labels.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## We calculate lengths of contexts\n",
    "context_lens = np.array([len(s) for s in contexts])\n",
    "\n",
    "## We calculate the total size of the text \n",
    "context_N = sum(c_lens)\n",
    "\n",
    "## And the number of contexts (users that viewed two or more items -- can be even the same items)\n",
    "len_context = len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size = 32, f = lambda x : x): \n",
    "    \"\"\"\n",
    "    Generates batch, labels (as in tf function)\n",
    "    More advanced version\n",
    "    \n",
    "    Arguments: \n",
    "    batch_size -- size of the batch\n",
    "    w_func -- how to account for the different sizes of contexts -- we will \n",
    "              choose a batch word from context of size n with probability f(n/N)\n",
    "              where N is the sum of sizes of all contexts \n",
    "    \n",
    "    Return: \n",
    "    batch -- words from which we try to predict labels\n",
    "    labels -- words that we are trying to predict from batch\n",
    "    \"\"\"\n",
    "    ## Calculate probability of selecting each context \n",
    "    p_select_context = f(context_lens/context_N)\n",
    "    ## Multinomial distribution, distributing batch_size items to groups of 0 .. (len_c - 1)\n",
    "    ## with probabilities p_select_context\n",
    "    how_many_words_from_context = np.random.multinomial(batch_size, p_select_context)\n",
    "    arg_nonzero = np.argwhere(how_many_words_from_context)\n",
    "    how_many_nonzero = how_many_words_from_context[arg_nonzero]\n",
    "    \n",
    "    ----- TODO -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build Tensorflow model ** \n",
    "\n",
    "This is modified copy of official tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Size of training batch\n",
    "batch_size = 32\n",
    "\n",
    "## Length of embedding vectors \n",
    "embedding_size = 32\n",
    "\n",
    "## Number of negative examples, see nonnegative sampling \n",
    "num_sampled = 16 \n",
    "\n",
    "## Number of iterations of optimization algorithm\n",
    "max_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Number of distinct words in vocabulary\n",
    "vocabulary_size = len(user_item_df[\"item\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "# Look up embeddings for inputs.\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# Construct the variables for the NCE loss\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Compute the average NCE loss for the batch.\n",
    "# tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "# time we evaluate the loss.\n",
    "# Explanation of the meaning of NCE loss:\n",
    "#   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(weights=nce_weights,\n",
    "                   biases=nce_biases,\n",
    "                   labels=train_labels,\n",
    "                   inputs=embed,\n",
    "                   num_sampled=num_sampled,\n",
    "                   num_classes=vocabulary_size))\n",
    "\n",
    "# Construct the SGD optimizer using a learning rate of 1.0.\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "# Add variable initializer.\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Learn the embeddings\n",
    "for i in range(max_iter): \n",
    "    batch, labels = generate_batch(batch_size)\n",
    "    sess.run(optimizer, {train_inputs : batch, train_labels : labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_run = sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prediction ** \n",
    "\n",
    "I have many word embeddings and I need to predict distribution of that user. \n",
    "\n",
    "Just calculate the average embedding x and calculate softmax(Wx + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--- TODO -- promyslet trochu jeste predchozi bunku "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
