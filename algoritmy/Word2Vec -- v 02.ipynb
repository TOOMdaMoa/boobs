{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "import pprint\n",
    "\n",
    "## My libraries \n",
    "import comms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of w2v to (user - product) pairs \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load Raw Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = comms.load_jsons() ## list of jsons, TESTING:    pprint.pprint(data[0], depth=1)\n",
    "\n",
    "user_item_df = comms.user_item_dataframe(data) ## dataframe [\"context\", \"word\"] = [\"user\", \"product\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Transform the data to list of item - item pairs **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bags, context_ids = comms.create_contexts(user_item_df) ## list of lists of product_ids, list of user_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create item dictionary **\n",
    "\n",
    "For w2v it is important to order the words by their frequency in text. \n",
    "In text processing applications this is obvious, but here \n",
    "there are two options how to count occurence of each word (product).\n",
    "- a) by number of users that viewed it  \n",
    "- b) by frequency in word_context_pairs -- this will higly push upward items that viewed someone who viewed a lot of items\n",
    "\n",
    "** Warning **\n",
    "Till now we worked with words as strings/ints - from now on words will be marked only by index in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word_context_pairs -- pairs of (product, product) that occured in views of some user\n",
    "## dictionary -- list of \"item_id\" : frequency -- frequency of word x is 3 means x is third most frequent word \n",
    "## reversed_dictionary -- frequency : \"item_id\"\n",
    "dictionary, reversed_dictionary = comms.create_dictionary(word_bags, \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Batch generating functions  **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genbatch_prereq = comms.create_genbatch_prerequisities(word_bags, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size): \n",
    "    return comms.generate_batch(batch_size, genbatch_prereq, \"A\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build Tensorflow model and optimize it ** \n",
    "\n",
    "This is modified copy of official tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Size of training batch\n",
    "batch_size = 32\n",
    "\n",
    "## Length of embedding vectors \n",
    "embedding_size = 32\n",
    "\n",
    "## Number of negative examples, see nonnegative sampling \n",
    "num_sampled = 16 \n",
    "\n",
    "## Number of iterations of optimization algorithm\n",
    "num_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of distinct words in vocabulary\n",
    "vocabulary_size = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = comms.create_w2v_tf_model(batch_size, embedding_size, vocabulary_size, num_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_labels, embeddings, nce_weights, nce_biases, loss, optimizer, init = w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn the embeddings\n",
    "for i in range(num_iter): \n",
    "    batch, labels = generate_batch(batch_size)\n",
    "    sess.run(optimizer, {train_inputs : batch, train_labels : labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_eval, nce_weights_eval, nce_biases_eval = sess.run([embeddings, nce_weights, nce_biases]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape \t (25, 32)\n",
      "nce weights shape \t (25, 32)\n",
      "nce biases shape \t (25,)\n"
     ]
    }
   ],
   "source": [
    "print(\"embeddings shape \\t\", embeddings_eval.shape)\n",
    "print(\"nce weights shape \\t\", nce_weights_eval.shape)\n",
    "print(\"nce biases shape \\t\", nce_biases_eval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prediction ** \n",
    "\n",
    "I have many word embeddings and I need to predict distribution of that user. \n",
    "\n",
    "Just calculate the average embedding x and calculate softmax(Wx + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'730aa47b6abdb6a103df165ba9a69ce7'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_ids[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction by user id \n",
    "user_id = \"730aa47b6abdb6a103df165ba9a69ce7\"\n",
    "ind = context_ids.index(user_id)\n",
    "user_embed = np.mean(embeddings_eval[[dictionary[word] for word in word_bags[ind]], :], axis = 0)\n",
    "prob_new_word = comms.softmax(np.sum(nce_weights_eval * user_embed, axis=1) + nce_biases_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
