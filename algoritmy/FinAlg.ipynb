{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## General libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "import pprint\n",
    "import pickle\n",
    "\n",
    "## My libraries \n",
    "import comms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Optimization Settings  \n",
    "counts_alg = \"A\"\n",
    "genbatch_alg = \"A\"\n",
    "batch_size = 32\n",
    "embedding_size = 32\n",
    "num_sampled = 16\n",
    "num_iter = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load and format data to sessions and make test and train set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_bags_train, word_bags_train, context_ids_train = pickle.load( open( \"train_sessions.pkl\", \"rb\" ) )\n",
    "date_bags_test, word_bags_test, context_ids_test = pickle.load( open( \"test_sessions.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Generate data for training the model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 1) context == all data of one user \n",
    "word_bags_all_per_user = [[w for wb in word_bag for w in wb] for word_bag in word_bags_train] ## np.sum(np.array([len(w)<2 for w in word_bags_all_per_user]))\n",
    "\n",
    "## 2) context = one user session \n",
    "word_bags_all_per_sess = [wb for word_bag in word_bags_train for wb in word_bag]\n",
    "word_bags_all_per_sess = [wb for wb in word_bags_all_per_sess if len(wb) >= 2] ## np.sum(np.array([len(x)<2 for x in word_bags_all_per_sess]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dict and batch gen **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary, reversed_dictionary = comms.create_dictionary(word_bags_all_per_sess, counts_alg)\n",
    "\n",
    "## Number of distinct words in vocabulary\n",
    "vocabulary_size = len(dictionary)\n",
    "\n",
    "genbatch_prereq = comms.create_genbatch_prerequisities(word_bags_all_per_sess, dictionary)\n",
    "\n",
    "def generate_batch(batch_size): \n",
    "    return comms.generate_batch(batch_size, genbatch_prereq, genbatch_alg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build model and train the shit out of it **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = comms.create_w2v_tf_model(batch_size, embedding_size, vocabulary_size, num_sampled, learn_rate = 1.0)\n",
    "train_inputs, train_labels, embeddings, nce_weights, nce_biases, loss, optimizer, init = w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0  :  46.1317\n",
      "Loss at iteration 1000  :  9.94793\n",
      "Loss at iteration 2000  :  2.34709\n",
      "Loss at iteration 3000  :  3.91517\n",
      "Loss at iteration 4000  :  2.86745\n",
      "Loss at iteration 5000  :  2.60428\n",
      "Loss at iteration 6000  :  1.99643\n",
      "Loss at iteration 7000  :  2.67833\n",
      "Loss at iteration 8000  :  2.04484\n",
      "Loss at iteration 9000  :  2.00315\n",
      "Loss at iteration 10000  :  1.81386\n",
      "Loss at iteration 11000  :  1.91279\n",
      "Loss at iteration 12000  :  1.85742\n",
      "Loss at iteration 13000  :  1.5301\n",
      "Loss at iteration 14000  :  2.19321\n",
      "Loss at iteration 15000  :  1.3974\n",
      "Loss at iteration 16000  :  2.06971\n",
      "Loss at iteration 17000  :  1.3598\n",
      "Loss at iteration 18000  :  1.36862\n",
      "Loss at iteration 19000  :  1.75103\n",
      "Loss at iteration 20000  :  1.67599\n",
      "Loss at iteration 21000  :  2.51529\n",
      "Loss at iteration 22000  :  1.14279\n",
      "Loss at iteration 23000  :  1.35938\n",
      "Loss at iteration 24000  :  1.2127\n",
      "Loss at iteration 25000  :  1.44746\n",
      "Loss at iteration 26000  :  1.24138\n",
      "Loss at iteration 27000  :  0.842758\n",
      "Loss at iteration 28000  :  0.830624\n",
      "Loss at iteration 29000  :  1.55262\n",
      "Loss at iteration 30000  :  1.20865\n",
      "Loss at iteration 31000  :  0.821796\n",
      "Loss at iteration 32000  :  1.08145\n",
      "Loss at iteration 33000  :  1.38675\n",
      "Loss at iteration 34000  :  1.41305\n",
      "Loss at iteration 35000  :  1.34116\n",
      "Loss at iteration 36000  :  1.47045\n",
      "Loss at iteration 37000  :  0.983052\n",
      "Loss at iteration 38000  :  1.50113\n",
      "Loss at iteration 39000  :  1.31263\n",
      "Loss at iteration 40000  :  1.12464\n",
      "Loss at iteration 41000  :  1.15212\n",
      "Loss at iteration 42000  :  1.65358\n",
      "Loss at iteration 43000  :  0.960664\n",
      "Loss at iteration 44000  :  2.02051\n",
      "Loss at iteration 45000  :  1.68874\n",
      "Loss at iteration 46000  :  0.997537\n",
      "Loss at iteration 47000  :  1.12969\n",
      "Loss at iteration 48000  :  1.3368\n",
      "Loss at iteration 49000  :  1.56736\n"
     ]
    }
   ],
   "source": [
    "## Learn the embeddings\n",
    "for i in range(num_iter): \n",
    "    batch, labels = generate_batch(batch_size)\n",
    "    _, loss_eval = sess.run([optimizer, loss], {train_inputs : batch, train_labels : labels})\n",
    "    if i % 1000 == 0: \n",
    "        print(\"Loss at iteration\",i, \" : \",loss_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_eval, nce_weights_eval, nce_biases_eval = sess.run([embeddings, nce_weights, nce_biases]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Test that shit **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wb_two_sess = [wb for wb in word_bags_test if len(wb)>=2]\n",
    "wb_test_flat = [[w for wb in word_bag for w in wb] for word_bag in word_bags_test]  ## np.sum(np.array([len(x)<2 for x in wb_test_flat])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = wb_test_flat[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_order(ind_list):\n",
    "    \"\"\"\n",
    "    Order in which we will present item i to the user \n",
    "    \n",
    "    Arguments:\n",
    "    ind_list -- indices of items according to dictionary\n",
    "    \n",
    "    Return:\n",
    "    order_presented -- vector of shape (embeddings.shape[0], ), ith entry is order in which we present ith item\n",
    "    \"\"\"\n",
    "    mean_emb = np.mean(embeddings_eval[ind_list,:], axis=0)\n",
    "    preferences = comms.softmax(np.sum(nce_weights_eval * mean_emb, axis=1) + nce_biases_eval)\n",
    "    order_presented = len(preferences) - np.argsort(np.argsort(preferences)) \n",
    "    return order_presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_one_sess(sess, naive=False): \n",
    "    \"\"\"\n",
    "    sess -- list of item ids\n",
    "    naive -- use naive approach for defining order, we present items that were already selected\n",
    "             returns order 1 if predicted item is in past items\n",
    "             returns 1000 if predicted item is not in past items\n",
    "    \n",
    "    \"\"\"\n",
    "    ind = [dictionary.get(s) for s in sess]\n",
    "    ind = [i for i in ind if i != None]\n",
    "    if len(ind) == 0:\n",
    "        return []\n",
    "    else: \n",
    "        sess_orders = []\n",
    "        for j in range(1, len(ind)-1):\n",
    "#             print(j,\"  \", ind[0:j])\n",
    "            if naive: \n",
    "                sess_orders.append(1 if ind[j] in ind[0:j] else 1000)\n",
    "            else:\n",
    "                sess_orders.append(pred_order(ind[0:j])[ind[j]])\n",
    "    return sess_orders\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evals_for_flat_users = [eval_one_sess(s, naive=False) for s in wb_test_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0089999999999999"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile([1,2,3,4,5,6,7,8,9,10], q = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing users:  500\n",
      "Number of testing users, where prediction available: 216\n",
      "Number of users that had more than half of their predictions on first page:  213\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of testing users: \", len(wb_test_flat))\n",
    "print(\"Number of testing users, where prediction available:\", np.sum(np.array([len(x) == 0 for x in evals_for_flat_users])))\n",
    "median_evals_for_flat_users = [np.median(e) for e in evals_for_flat_users if len(e) > 0]\n",
    "print(\"Number of users that had more than half of their predictions on first page: \", np.sum(np.array([e <= 48 for e in median_evals_for_flat_users])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861111111111112"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "213/216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([e <= 48 for e in evals_for_flat_users]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(median_evals_for_flat_users, range=[0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
